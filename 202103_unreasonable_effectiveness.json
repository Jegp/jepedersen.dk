{"slug":"202103_unreasonable_effectiveness","metadata":{"title":"The unreasonable effectiveness of deep learning","pubdate":"2021/11/20","image":"deep_net.png","bibliography":"bib.bib","dateString":"Sat Nov 20 2021"},"html":"<p>The search for building and understanding <em>intelligent machines</em> has\nbeen under way for thousands of years and appear fundamental to the\nhuman condition [<a href=\"#Russel_Norvig_2010\">1</a>]. Deep learning models have\nrecently been heavily dominating this search for artificial intelligence\n(AI), as demonstrated by the go-playing Alpha Go [<a href=\"#Silver2017\">2</a>], the\nnarrating GPT-3 [<a href=\"#Brown2020\">3</a>], and many more. This is an odd development\nfor many since computational graphs are old news, and because no\nground-breaking heureka moment has guided the development. Rather, deep\nlearning is guided by a primitive brute-force optimization mechanism.</p>\n<p>Why is it that deep learning and deep neural networks have become so\npopular despite their simplicity? If there is no theoretical foundation\nto build on, why are they so <em>unreasonably</em> efficient?</p>\n<p>In this essay, I will argue the answer is: part coincidence, part skill.\nMotivating the skillfulness, I will first define deep learning (DL) and\nattempt to explain the phenomenon from a first-principle&#39;s approach. In\nthis light, I will describe the &quot;coincidental&quot; part by revisiting sets\nof problems DL is particularly good at, so we can finally arrive at an\nanswer to our question: why are deep networks so useful?</p>\n<h1 id=\"deep-learning\">Deep Learning</h1>\n<p>Even though many DL proponents promote its similarities to the human\nbrain, the building block of deep networks have little to do with\nnervous systems. Deep networks can be roughly defined as large\ncomputational graphs with weighted nonlinearities.</p>\n<p>This [<a href=\"#Carson_1925\">4</a>] is [<a href=\"#Shannon1942\">5</a>] not [<a href=\"#Rosenblatt_1958\">6</a>] new\n[<a href=\"#Rumelhart_Hinton_Williams_1986\">7</a>].</p>\n<p>In fact, eerily similar approaches was attempted throughout 1960-1990,\nwith little success. What makes deep networks interesting, though, is\nthat they can approximate <em>any function</em> to <em>any desired degree of\napproximation</em> [<a href=\"#Hornik_Stinchcombe_White_1989\">8</a>]. This striking fact was\ndiscovered in 1986 and tells us that deep networks can solve <em>any</em>\nproblem, <em>if</em> they are big enough. Granted, that is a big <em>if</em>, and this\nhas largely been driving the explosive growth of the graphics processing\nunits (GPUs) market: the bigger network, the better approximations.</p>\n<p>It still took several decades before the idea of a universal function\napproximator could be used to solve problems with any form of practical\nrelevance. Theoretically, this was already solved in 1986,\nwhere <a href=\"#Rumelhart_Hinton_Williams_1986\">7</a>\nshowed that large nets could be &quot;trained&quot; by <em>searching</em> for optimal\nconfigurations in a sea of possibilities. However, it was not until the\nearly 2000&#39;s that sufficiently powerful hardware came along to build and\nsearch through massive deep networks now up to <em>hundreds of trillions</em>\nof parameters [<a href=\"#Brown2020\">3</a>].</p>\n<figure>\n<img src=\"cat.png\"/>\n<p class=\"caption\">\nFigure 1. Two possible classification task based on the same cat picture: \nobject identification (top) and masking out pixels belonging to the cat (bottom).\n</p>\n</figure>\n\n<h1 id=\"deep-learning-first-principles\">Deep Learning first principles</h1>\n<p>It may come as a surprise how this type of searching yields meaningful\nnetworks. Fortunately, physicists and mathematicians helped resolve\nparts of the conundrum with two ancient ideas: symmetry\n[<a href=\"#Gens_Domingos_2014\">9</a>] and equivariance [<a href=\"#Cohen_Welling_2016\">10</a>].</p>\n<p>It turns out that symmetry is a useful property during learning. If you\nare to, say, identify a cat in an image, it should not matter how big\nthe cat is. Or where in the picture the cat is. In other words, we\nwant to find a way to learn a way to look at cats that are <em>symmetric</em>\nunder scale (size) or translation (location in picture). This symmetry\nis normally called <em>invariance</em>, because the idea that a cat is in the\npicture <em>does not vary</em> if the cat is very large and dead-center, or \nreally small in the lower left corner. We identify cats independently\nfrom those <em>distortions</em>, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>ξ</mi></mrow><annotation encoding=\"application/x-tex\">\\xi</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.04601em;\">ξ</span></span></span></span>.</p>\n<p>We can write this down more precisely by looking at some images\n<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi><mo>∈</mo><mi mathvariant=\"double-struck\">I</mi></mrow><annotation encoding=\"application/x-tex\">x \\in \\mathbb{I}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5782em;vertical-align:-0.0391em;\"></span><span class=\"mord mathnormal\">x</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6889em;\"></span><span class=\"mord mathbb\">I</span></span></span></span>\nthat we are looking to classify into a binary decision\n<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi><mo>∈</mo><mi mathvariant=\"double-struck\">C</mi><mo>:</mo><mo stretchy=\"false\">{</mo><mtext>CAT</mtext><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">¬</mi><mtext>CAT</mtext><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">y \\in \\mathbb{C}: \\{\\text{CAT}, \\neg\\text{CAT}\\}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7335em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6889em;\"></span><span class=\"mord mathbb\">C</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">:</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">{</span><span class=\"mord text\"><span class=\"mord\">CAT</span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">¬</span><span class=\"mord text\"><span class=\"mord\">CAT</span></span><span class=\"mclose\">}</span></span></span></span> using\na network <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>f</mi><mo>:</mo><mi mathvariant=\"double-struck\">I</mi><mo>↦</mo><mi mathvariant=\"double-struck\">C</mi></mrow><annotation encoding=\"application/x-tex\">f: \\mathbb{I} \\mapsto \\mathbb{C}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">:</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6999em;vertical-align:-0.011em;\"></span><span class=\"mord mathbb\">I</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">↦</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6889em;\"></span><span class=\"mord mathbb\">C</span></span></span></span>, we can therefore say that\nthe guess <em>without</em> a distortion <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>ξ</mi><mo>:</mo><mi mathvariant=\"double-struck\">I</mi><mo>↦</mo><mi mathvariant=\"double-struck\">I</mi></mrow><annotation encoding=\"application/x-tex\">\\xi: \\mathbb{I} \\mapsto \\mathbb{I}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.04601em;\">ξ</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">:</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6999em;vertical-align:-0.011em;\"></span><span class=\"mord mathbb\">I</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">↦</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6889em;\"></span><span class=\"mord mathbb\">I</span></span></span></span>,\nshould be the same <em>with</em> the distortion: <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>f</mi><mo stretchy=\"false\">(</mo><mi>ξ</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">f(x) = f(\\xi (x))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.04601em;\">ξ</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">))</span></span></span></span></p>\n<p>We can take this one step further by generalizing the invariance, so\nthat the networks can operate on distorted images, <em>while retaining the\ndistortion</em> [<a href=\"#Cohen_Welling_2016\">10</a>]. Imagine for a while, that we are no\nlonger identifying cats, but colouring them, as shown in Figure 1. In\nthat case, invariance will not do because we need to <em>include the\ndistortion in the output</em>. Relating this to the example of the cat, we\ncannot always think that the cat is in the center of the image and\ncolour its outline there. Rather, we need to correctly scale and\ntranslate the outline in the output image, that is, we need to <em>preserve\nthe structure</em> of the distortion. \nWhen we want to mask our cat (as in Figure 1), our <em>structure preserving</em>\nnetwork should be written as <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>f</mi><mo>:</mo><mi mathvariant=\"double-struck\">I</mi><mo>↦</mo><mi mathvariant=\"double-struck\">I</mi></mrow><annotation encoding=\"application/x-tex\">f: \\mathbb{I} \\mapsto \\mathbb{I}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">:</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6999em;vertical-align:-0.011em;\"></span><span class=\"mord mathbb\">I</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">↦</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6889em;\"></span><span class=\"mord mathbb\">I</span></span></span></span> and \nfulfill<a href=\"#footnote-1\" class=\"footnote-a\">1</a>: <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>ξ</mi><mo stretchy=\"false\">(</mo><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo>=</mo><mi>f</mi><mo stretchy=\"false\">(</mo><mi>ξ</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\xi(f(x)) = f(\\xi(x))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.04601em;\">ξ</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">))</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.04601em;\">ξ</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">))</span></span></span></span></p>\n<p>Our network is now a <em>structure preserving map</em>, also known as <i><a href=\"https://en.wikipedia.org/wiki/Equivariant_map\">equivariant maps</a></i>.\nThey are fundamental to, for instance, topology and graph theory.</p>\n<h1 id=\"suitable-problems-for-deep-learning\">Suitable problems for Deep Learning</h1>\n<p>It has been shown that deep networks are particularly good at preserving\nsymmetry and structures [<a href=\"#Cohen_Welling_2016\">10</a>; <a href=\"#Gens_Domingos_2014\">9</a>]. This\nkind of preservation allows for the detection of symmetries and\nstructures, but also the <em>mapping</em> of symmetries and structures between\ndomains such as image, 3d-space, language, audio and many others\n[<a href=\"#Bronstein_Bruna_Cohen_Velickovic_2021\">11</a>].</p>\n<p>This is obviously relevant for classifying and masking cats, but what about\neverything else &quot;AI&quot;? What about other types of\nintelligence like bodily intelligences, causal reasoning (what is the\n<em>source</em> of the structure), abstract thinking, and so on? Is that all a\nmatter of preserving mapping? \nMaybe. We do not know [<a href=\"#Saxe_Nelli_Summerfield_2021\">12</a>]. Please find out.</p>\n<figure>\n<img src=\"ai_pop.png\" style=\"width: 80%;\"/>\n<p class=\"caption\">\nPopularity of the search terms \"deep learning\" and \"artificial\nintelligence\" on Google Trends [<a href=\"#Google2021\">13</a>]. The Y-axis expresses\nrelative popularity where 100 indicates the peak number of\nsearchers.\n</p>\n</figure>\n\n<h1 id=\"why-did-deep-learning-become-so-popular\">Why did Deep Learning become so popular?</h1>\n<p>Returning to the idea of a universal function approximator, these\nnetworks are precisely exercising the approximation of domain mappings.\nThey find appropriate responses such as &quot;CAT&quot;, a colouring mask, or even\na synthesized voice command. Pause here for a second and imagine how\nmany problems can be solved this way. Is language a mapping of concepts\nto voice or text [<a href=\"#Brown2020\">3</a>]? Is programming a mapping of concepts to\ncode [<a href=\"#Tiwang_Oladunni_Xu_2019\">14</a>]?</p>\n<p>By now, the reader is hopefully convinced of the usefulness of the\napproach. And plenty of evidence exists in applications for speech\nsynthesis, self-driving cars, facial recognition, object detection, \n<a href=\"https://artsandculture.google.com/experiment/blob-opera/AAHWrq360NcGbw\">fake blob opera</a>, etc.\nThis is the coincidence: pioneers of the abovementioned principles could\nnot possibly have known the extend to which their findings would\ngeneralize.</p>\n<p>Figure 2 plots the relative popularity of two search terms: &quot;artificial\nintelligence&quot; and &quot;deep learning&quot; [<a href=\"#Google2021\">13</a>]. It appears the general\npublic lost interest in &quot;classical&quot; AI in the beginning of the 00&#39;s only\nto slowly move towards deep learning-driven AI in the mid 10&#39;s.\nInterestingly enough, this coincides with the first papers on\ninvariances and structure preservations in 2014/15.</p>\n<p>It would be presumptuous to exclude other factors, and the advent of\ncommercialized hardware that unlocks the training of large deep nets, is\ncertainly one such influence. But the fundamental properties we have\ndiscussed here, are not only interesting because they explain what is\nfeasible, they also hint at what is <em>not</em>. Perhaps that is why deep\nlearning is losing popularity towards the end of 2020?</p>\n<div class=\"footnotes\">\n[ <b id=\"footnote-1\">1</b> ]: This is also known as *equivariance* and the formula is slightly\n    more involved, since the distortion in the input domain may be\n    different from the distortion in the codomain. See\n    [<a href=\"#Bronstein_Bruna_Cohen_Velickovic_2021\">11</a>] for a more detailed\n    treatment.\n\n</div>\n<h2>References</h2><ol class='references'><li id=\"#0\"><a href=\"http://aima.cs.berkeley.edu/\">Russel, Stuart and Norvig, Peter: <i>Artificial Intelligence: A Modern Approach, 4th US ed.</i>, 2010, Pearson</a></li><li id=\"#1\"><a href=\"https://doi.org/10.1038/nature24270\">Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis: <i>Mastering the game of Go without human knowledge</i>, 2017, Nature</a></li><li id=\"#2\"><a href=\"http://arxiv.org/abs/2005.14165\">Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario: <i>Language Models are Few-Shot Learners</i>, 2020, arXiv:2005.14165 [cs]</a></li><li id=\"#3\"><a href=\"https://doi.org/10.1002/j.1538-7305.1925.tb03972.x\">Carson, John R.: <i>Electric circuit theory and the operational calculus</i>, 1925, The Bell System Technical Journal</a></li><li id=\"#4\"><a href=\"https://ieeexplore.ieee.org/document/5311556\">Claude E. Shannon: <i>Claude E. Shannon: Collected Papers</i>, 1993, IEEE</a></li><li id=\"#5\"><a href=\"https://doi.org/10.1037/h0042519\">Rosenblatt, F.: <i>The perceptron: A probabilistic model for information storage and organization in the brain</i>, 1958, Psychological Review</a></li><li id=\"#6\"><a href=\"https://doi.org/10.1038/323533a0\">Rumelhart, D. and Hinton, Geoffrey E. and Williams, Ronald J.: <i>Learning representations by back-propagating errors</i>, 1986, Nature</a></li><li id=\"#7\"><a href=\"https://doi.org/10.1016/0893-6080(89)90020-8\">Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert: <i>Multilayer feedforward networks are universal approximators</i>, 1989, Neural Networks</a></li><li id=\"#8\"><a href=\"https://papers.nips.cc/paper/2014/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.html\">Gens, Robert and Domingos, Pedro M: <i>Deep Symmetry Networks</i>, 2014, Advances in Neural Information Processing Systems</a></li><li id=\"#9\"><a href=\"http://arxiv.org/abs/1602.07576\">Cohen, Taco S. and Welling, Max: <i>Group Equivariant Convolutional Networks</i>, 2016, arXiv:1602.07576 [cs, stat]</a></li><li id=\"#10\"><a href=\"http://arxiv.org/abs/2104.13478\">Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar: <i>Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges</i>, 2021, arXiv:2104.13478 [cs, stat]</a></li><li id=\"#11\"><a href=\"https://doi.org/10.1038/s41583-020-00395-8\">Saxe, Andrew and Nelli, Stephanie and Summerfield, Christopher: <i>If deep learning is the answer, what is the question?</i>, 2021, Nature Reviews Neuroscience</a></li><li id=\"#12\">Google: <i>Google Trends</i>, 2021, undefined</li><li id=\"#13\"><a href=\"https://doi.org/10.1109/SoutheastCon42311.2019.9020360\">Tiwang, Raymond and Oladunni, Timothy and Xu, Weifeng: <i>A Deep Learning Model for Source Code Generation</i>, 2019, 2019 SoutheastCon</a></li></ol>"}