<!doctype html> <html lang=en> <head> <meta charset=utf-8> <meta content="width=device-width,initial-scale=1" name=viewport> <meta content=#333333 name=theme-color> <base href=/ > <link href=global.css rel=stylesheet> <link href=manifest.json rel=manifest crossorigin=use-credentials> <link href=favicon.png rel=icon type=image/png> <link href=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css rel=stylesheet crossorigin=anonymous integrity=sha384-t5CR+zwDAROtph0PXGte6ia8heboACF9R5l/DiY+WZ3P2lxNgvJkQk5n7GPvLMYw> <link href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css rel=stylesheet crossorigin=anonymous integrity="sha512-MV7K8+y+gLIBoVD59lQIYicR65iaqukzvf/nwasF0nqhPay5w/9lJmVM2hMDcnK1OnMGCdVK+iQrJ7lzPJQd1w==" referrerpolicy=no-referrer> <link href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/brands.min.css rel=stylesheet crossorigin=anonymous integrity="sha512-G/T7HQJXSeNV7mKMXeJKlYNJ0jrs8RsWzYG7rVACye+qrcUhEAYKYzaa+VFy6eFzM2+/JT1Q+eqBbZFSHmJQew==" referrerpolicy=no-referrer> <script crossorigin=anonymous defer integrity=sha384-FaFLTlohFghEIZkw6VGwmf9ISTubWAVYW8tG8+w2LAIftJEULZABrF9PPFv+tVkH src=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js></script> <link href=client/client-4812616f.css rel=stylesheet><link href=client/_slug_-d8485549.css rel=stylesheet> <title>The unreasonable effectiveness of deep learning</title> <link href=/client/client.7949dabb.js rel=modulepreload crossorigin=use-credentials as=script><link href=/client/client-4812616f.css rel=preload as=style><link href=/client/_slug_.6690e68e.js rel=modulepreload crossorigin=use-credentials as=script><link href=/client/inject_styles.803b7e80.js rel=modulepreload crossorigin=use-credentials as=script><link href=/client/_slug_-d8485549.css rel=preload as=style></head> <body> <div id=sapper> <nav class=svelte-19e8481><ul class=svelte-19e8481><li class=svelte-19e8481><a href=. class="svelte-19e8481 deselected">J. E. Pedersen</a></li> <li class="svelte-19e8481 fill"></li> <li class=svelte-19e8481><a href=. class=svelte-19e8481>home</a></li> <li class=svelte-19e8481><a href=about class=svelte-19e8481>about </a></li> <li class=svelte-19e8481><a href=https://mastodon.social/@jegp class=svelte-19e8481 title="Jens E. Pedersen on Mastodon"><i class="svelte-19e8481 fa-brands fa-mastodon"></i> <span class="svelte-19e8481 no-transform">@jegp</span></a></li> <li class=svelte-19e8481><a href=https://github.com/jegp class=svelte-19e8481 title="Jens E. Pedersen on GitHub"><i class="svelte-19e8481 fa-brands fa-github"></i> <span class="svelte-19e8481 no-transform">@jegp</span></a></ul> </nav> <main class=svelte-1xzg7fw> <figure class=svelte-gaan86><img src=deep_net.png alt="" class=svelte-gaan86> </figure> <section class="svelte-gaan86 content"><h1>The unreasonable effectiveness of deep learning</h1> <span class=date>2021/11/20</span> <p>The search for building and understanding <em>intelligent machines</em> has been under way for thousands of years and appear fundamental to the human condition [<a href=202111_unreasonable_effectiveness/#0>1</a>]. Deep learning models have recently been heavily dominating this search for artificial intelligence (AI), as demonstrated by the go-playing Alpha Go [<a href=202111_unreasonable_effectiveness/#1>2</a>], the narrating GPT-3 [<a href=202111_unreasonable_effectiveness/#2>3</a>], and many more. This is an odd development for many since computational graphs are old news, and because no ground-breaking heureka moment has guided the development. Rather, deep learning is guided by a primitive brute-force optimization mechanism.</p> <p>Why is it that deep learning and deep neural networks have become so popular despite their simplicity? If there is no theoretical foundation to build on, why are they so <em>unreasonably</em> efficient?</p> <p>In this essay, I will argue the answer is: part coincidence, part skill. Motivating the skillfulness, I will first define deep learning (DL) and attempt to explain the phenomenon from a first-principle's approach. In this light, I will describe the "coincidental" part by revisiting sets of problems DL is particularly good at, so we can finally arrive at an answer to our question: why are deep networks so useful?</p> <h1 id=deep-learning>Deep Learning</h1> <p>Even though many DL proponents promote its similarities to the human brain, the building block of deep networks have little to do with nervous systems. Deep networks can be roughly defined as large computational graphs with weighted nonlinearities.</p> <p>This [<a href=202111_unreasonable_effectiveness/#3>4</a>] is [<a href=202111_unreasonable_effectiveness/#4>5</a>] not [<a href=202111_unreasonable_effectiveness/#5>6</a>] new [<a href=202111_unreasonable_effectiveness/#6>7</a>].</p> <p>In fact, eerily similar approaches was attempted throughout 1960-1990, with little success. What makes deep networks interesting, though, is that they can approximate <em>any function</em> to <em>any desired degree of approximation</em> [<a href=202111_unreasonable_effectiveness/#7>8</a>]. This striking fact was discovered in 1986 and tells us that deep networks can solve <em>any</em> problem, <em>if</em> they are big enough. Granted, that is a big <em>if</em>, and this has largely been driving the explosive growth of the graphics processing units (GPUs) market: the bigger network, the better approximations.</p> <p>It still took several decades before the idea of a universal function approximator could be used to solve problems with any form of practical relevance. Theoretically, this was already solved in 1986, where <a href=202111_unreasonable_effectiveness/#6>7</a> showed that large nets could be "trained" by <em>searching</em> for optimal configurations in a sea of possibilities. However, it was not until the early 2000's that sufficiently powerful hardware came along to build and search through massive deep networks now up to <em>hundreds of trillions</em> of parameters [<a href=202111_unreasonable_effectiveness/#2>3</a>].</p> <figure> <img src=cat.png> <p class=caption> Figure 1. Two possible classification task based on the same cat picture: object identification (top) and masking out pixels belonging to the cat (bottom). </p> </figure> <h1 id=deep-learning-first-principles>Deep Learning first principles</h1> <p>It may come as a surprise how this type of searching yields meaningful networks. Fortunately, physicists and mathematicians helped resolve parts of the conundrum with two ancient ideas: symmetry [<a href=202111_unreasonable_effectiveness/#8>9</a>] and equivariance [<a href=202111_unreasonable_effectiveness/#9>10</a>].</p> <p>It turns out that symmetry is a useful property during learning. If you are to, say, identify a cat in an image, it should not matter how big the cat is. Or where in the picture the cat is. In other words, we want to find a way to learn a way to look at cats that are <em>symmetric</em> under scale (size) or translation (location in picture). This symmetry is normally called <em>invariance</em>, because the idea that a cat is in the picture <em>does not vary</em> if the cat is very large and dead-center, or really small in the lower left corner. We identify cats independently from those <em>distortions</em>, <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>ξ</mi></mrow><annotation encoding=application/x-tex>\xi</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.8889em;vertical-align:-.1944em></span><span class="mord mathnormal" style=margin-right:.04601em>ξ</span></span></span></span>.</p> <p>We can write this down more precisely by looking at some images <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>x</mi><mo>∈</mo><mi mathvariant=double-struck>I</mi></mrow><annotation encoding=application/x-tex>x \in \mathbb{I}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.5782em;vertical-align:-.0391em></span><span class="mord mathnormal">x</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>∈</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.6889em></span><span class="mord mathbb">I</span></span></span></span> that we are looking to classify into a binary decision <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>y</mi><mo>∈</mo><mi mathvariant=double-struck>C</mi><mo>:</mo><mo stretchy=false>{</mo><mtext>CAT</mtext><mo separator=true>,</mo><mi mathvariant=normal>¬</mi><mtext>CAT</mtext><mo stretchy=false>}</mo></mrow><annotation encoding=application/x-tex>y \in \mathbb{C}: \{\text{CAT}, \neg\text{CAT}\}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.7335em;vertical-align:-.1944em></span><span class="mord mathnormal" style=margin-right:.03588em>y</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>∈</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.6889em></span><span class="mord mathbb">C</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>:</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class=mopen>{</span><span class="mord text"><span class=mord>CAT</span></span><span class=mpunct>,</span><span class=mspace style=margin-right:.1667em></span><span class=mord>¬</span><span class="mord text"><span class=mord>CAT</span></span><span class=mclose>}</span></span></span></span> using a network <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>f</mi><mo>:</mo><mi mathvariant=double-struck>I</mi><mo>↦</mo><mi mathvariant=double-struck>C</mi></mrow><annotation encoding=application/x-tex>f: \mathbb{I} \mapsto \mathbb{C}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.8889em;vertical-align:-.1944em></span><span class="mord mathnormal" style=margin-right:.10764em>f</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>:</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.6999em;vertical-align:-.011em></span><span class="mord mathbb">I</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>↦</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.6889em></span><span class="mord mathbb">C</span></span></span></span>, we can therefore say that the guess <em>without</em> a distortion <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>ξ</mi><mo>:</mo><mi mathvariant=double-struck>I</mi><mo>↦</mo><mi mathvariant=double-struck>I</mi></mrow><annotation encoding=application/x-tex>\xi: \mathbb{I} \mapsto \mathbb{I}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.8889em;vertical-align:-.1944em></span><span class="mord mathnormal" style=margin-right:.04601em>ξ</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>:</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.6999em;vertical-align:-.011em></span><span class="mord mathbb">I</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>↦</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.6889em></span><span class="mord mathbb">I</span></span></span></span>, should be the same <em>with</em> the distortion: <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>f</mi><mo stretchy=false>(</mo><mi>x</mi><mo stretchy=false>)</mo><mo>=</mo><mi>f</mi><mo stretchy=false>(</mo><mi>ξ</mi><mo stretchy=false>(</mo><mi>x</mi><mo stretchy=false>)</mo><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>f(x) = f(\xi (x))</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class="mord mathnormal" style=margin-right:.10764em>f</span><span class=mopen>(</span><span class="mord mathnormal">x</span><span class=mclose>)</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class="mord mathnormal" style=margin-right:.10764em>f</span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:.04601em>ξ</span><span class=mopen>(</span><span class="mord mathnormal">x</span><span class=mclose>))</span></span></span></span></p> <p>We can take this one step further by generalizing the invariance, so that the networks can operate on distorted images, <em>while retaining the distortion</em> [<a href=202111_unreasonable_effectiveness/#9>10</a>]. Imagine for a while, that we are no longer identifying cats, but colouring them, as shown in Figure 1. In that case, invariance will not do because we need to <em>include the distortion in the output</em>. Relating this to the example of the cat, we cannot always think that the cat is in the center of the image and colour its outline there. Rather, we need to correctly scale and translate the outline in the output image, that is, we need to <em>preserve the structure</em> of the distortion. When we want to mask our cat (as in Figure 1), our <em>structure preserving</em> network should be written as <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>f</mi><mo>:</mo><mi mathvariant=double-struck>I</mi><mo>↦</mo><mi mathvariant=double-struck>I</mi></mrow><annotation encoding=application/x-tex>f: \mathbb{I} \mapsto \mathbb{I}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.8889em;vertical-align:-.1944em></span><span class="mord mathnormal" style=margin-right:.10764em>f</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>:</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.6999em;vertical-align:-.011em></span><span class="mord mathbb">I</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>↦</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.6889em></span><span class="mord mathbb">I</span></span></span></span> and fulfill<a href=#footnote-1 class=footnote-a>1</a>: <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>ξ</mi><mo stretchy=false>(</mo><mi>f</mi><mo stretchy=false>(</mo><mi>x</mi><mo stretchy=false>)</mo><mo stretchy=false>)</mo><mo>=</mo><mi>f</mi><mo stretchy=false>(</mo><mi>ξ</mi><mo stretchy=false>(</mo><mi>x</mi><mo stretchy=false>)</mo><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>\xi(f(x)) = f(\xi(x))</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class="mord mathnormal" style=margin-right:.04601em>ξ</span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:.10764em>f</span><span class=mopen>(</span><span class="mord mathnormal">x</span><span class=mclose>))</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class="mord mathnormal" style=margin-right:.10764em>f</span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:.04601em>ξ</span><span class=mopen>(</span><span class="mord mathnormal">x</span><span class=mclose>))</span></span></span></span></p> <p>Our network is now a <em>structure preserving map</em>, also known as <i><a href=https://en.wikipedia.org/wiki/Equivariant_map>equivariant maps</a></i>. They are fundamental to, for instance, topology and graph theory.</p> <h1 id=suitable-problems-for-deep-learning>Suitable problems for Deep Learning</h1> <p>It has been shown that deep networks are particularly good at preserving symmetry and structures [<a href=202111_unreasonable_effectiveness/#9>10</a>; <a href=202111_unreasonable_effectiveness/#8>9</a>]. This kind of preservation allows for the detection of symmetries and structures, but also the <em>mapping</em> of symmetries and structures between domains such as image, 3d-space, language, audio and many others [<a href=202111_unreasonable_effectiveness/#10>11</a>].</p> <p>This is obviously relevant for classifying and masking cats, but what about everything else "AI"? What about other types of intelligence like bodily intelligences, causal reasoning (what is the <em>source</em> of the structure), abstract thinking, and so on? Is that all a matter of preserving mapping? Maybe. We do not know [<a href=202111_unreasonable_effectiveness/#11>12</a>]. Please find out.</p> <figure> <img src=ai_pop.png style=width:80%> <p class=caption> Popularity of the search terms "deep learning" and "artificial intelligence" on Google Trends [<a href=202111_unreasonable_effectiveness/#12>13</a>]. The Y-axis expresses relative popularity where 100 indicates the peak number of searchers. </p> </figure> <h1 id=why-did-deep-learning-become-so-popular>Why did Deep Learning become so popular?</h1> <p>Returning to the idea of a universal function approximator, these networks are precisely exercising the approximation of domain mappings. They find appropriate responses such as "CAT", a colouring mask, or even a synthesized voice command. Pause here for a second and imagine how many problems can be solved this way. Is language a mapping of concepts to voice or text [<a href=202111_unreasonable_effectiveness/#2>3</a>]? Is programming a mapping of concepts to code [<a href=202111_unreasonable_effectiveness/#13>14</a>]?</p> <p>By now, the reader is hopefully convinced of the usefulness of the approach. And plenty of evidence exists in applications for speech synthesis, self-driving cars, facial recognition, object detection, <a href=https://artsandculture.google.com/experiment/blob-opera/AAHWrq360NcGbw>fake blob opera</a>, etc. This is the coincidence: pioneers of the abovementioned principles could not possibly have known the extend to which their findings would generalize.</p> <p>Figure 2 plots the relative popularity of two search terms: "artificial intelligence" and "deep learning" [<a href=202111_unreasonable_effectiveness/#12>13</a>]. It appears the general public lost interest in "classical" AI in the beginning of the 00's only to slowly move towards deep learning-driven AI in the mid 10's. Interestingly enough, this coincides with the first papers on invariances and structure preservations in 2014/15.</p> <p>It would be presumptuous to exclude other factors, and the advent of commercialized hardware that unlocks the training of large deep nets, is certainly one such influence. But the fundamental properties we have discussed here, are not only interesting because they explain what is feasible, they also hint at what is <em>not</em>. Perhaps that is why deep learning is losing popularity towards the end of 2020?</p> <div class=footnotes> [ <b id=footnote-1>1</b> ]: This is also known as *equivariance* and the formula is slightly more involved, since the distortion in the input domain may be different from the distortion in the codomain. See [<a href=202111_unreasonable_effectiveness/#10>11</a>] for a more detailed treatment. </div> <h2>References</h2><ol class=references><li id=0><a href=http://aima.cs.berkeley.edu/ >Russel, Stuart and Norvig, Peter: <i>Artificial Intelligence: A Modern Approach, 4th US ed.</i>, 2010, Pearson</a><li id=1><a href=https://doi.org/10.1038/nature24270>Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis: <i>Mastering the game of Go without human knowledge</i>, 2017, Nature</a><li id=2><a href=http://arxiv.org/abs/2005.14165>Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario: <i>Language Models are Few-Shot Learners</i>, 2020, arXiv:2005.14165 [cs]</a><li id=3><a href=https://doi.org/10.1002/j.1538-7305.1925.tb03972.x>Carson, John R.: <i>Electric circuit theory and the operational calculus</i>, 1925, The Bell System Technical Journal</a><li id=4><a href=https://ieeexplore.ieee.org/document/5311556>Claude E. Shannon: <i>Claude E. Shannon: Collected Papers</i>, 1993, IEEE</a><li id=5><a href=https://doi.org/10.1037/h0042519>Rosenblatt, F.: <i>The perceptron: A probabilistic model for information storage and organization in the brain</i>, 1958, Psychological Review</a><li id=6><a href=https://doi.org/10.1038/323533a0>Rumelhart, D. and Hinton, Geoffrey E. and Williams, Ronald J.: <i>Learning representations by back-propagating errors</i>, 1986, Nature</a><li id=7><a href=https://doi.org/10.1016/0893-6080(89)90020-8>Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert: <i>Multilayer feedforward networks are universal approximators</i>, 1989, Neural Networks</a><li id=8><a href=https://papers.nips.cc/paper/2014/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.html>Gens, Robert and Domingos, Pedro M: <i>Deep Symmetry Networks</i>, 2014, Advances in Neural Information Processing Systems</a><li id=9><a href=http://arxiv.org/abs/1602.07576>Cohen, Taco S. and Welling, Max: <i>Group Equivariant Convolutional Networks</i>, 2016, arXiv:1602.07576 [cs, stat]</a><li id=10><a href=http://arxiv.org/abs/2104.13478>Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar: <i>Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges</i>, 2021, arXiv:2104.13478 [cs, stat]</a><li id=11><a href=https://doi.org/10.1038/s41583-020-00395-8>Saxe, Andrew and Nelli, Stephanie and Summerfield, Christopher: <i>If deep learning is the answer, what is the question?</i>, 2021, Nature Reviews Neuroscience</a><li id=12>Google: <i>Google Trends</i>, 2021, undefined<li id=13><a href=https://doi.org/10.1109/SoutheastCon42311.2019.9020360>Tiwang, Raymond and Oladunni, Timothy and Xu, Weifeng: <i>A Deep Learning Model for Source Code Generation</i>, 2019, 2019 SoutheastCon</a></ol></section> <div><style>h2{margin:2rem .3rem}.content figure{border:1px solid #333;border-left:0;border-right:0;text-align:center;padding:.5em 0}.content img{max-width:100%;margin:0 auto}.content .caption{font-size:80%}.footnote-a{vertical-align:super;margin:3px}.footnotes{font-size:70%}</style> </div></main> <footer class=svelte-1xzg7fw><p>Except where otherwise noted, content on this site is licensed under a  <a href=https://creativecommons.org/licenses/by/4.0/ title="Creative Commons Attribution 4.0 International license" rel=license>Creative Commons Attribution 4.0 International license </a>. </p> </footer></div> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,{post:{slug:"202111_unreasonable_effectiveness",metadata:{title:"The unreasonable effectiveness of deep learning",pubdate:"2021\u002F11\u002F20",image:"deep_net.png",bibliography:"bib.bib",dateString:"Sat Nov 20 2021"},html:"\u003Cp\u003EThe search for building and understanding \u003Cem\u003Eintelligent machines\u003C\u002Fem\u003E has\nbeen under way for thousands of years and appear fundamental to the\nhuman condition [\u003Ca href=\"202111_unreasonable_effectiveness\u002F#0\"\u003E1\u003C\u002Fa\u003E]. Deep learning models have\nrecently been heavily dominating this search for artificial intelligence\n(AI), as demonstrated by the go-playing Alpha Go [\u003Ca href=\"202111_unreasonable_effectiveness\u002F#1\"\u003E2\u003C\u002Fa\u003E], the\nnarrating GPT-3 [\u003Ca href=\"202111_unreasonable_effectiveness\u002F#2\"\u003E3\u003C\u002Fa\u003E], and many more. This is an odd development\nfor many since computational graphs are old news, and because no\nground-breaking heureka moment has guided the development. Rather, deep\nlearning is guided by a primitive brute-force optimization mechanism.\u003C\u002Fp\u003E\n\u003Cp\u003EWhy is it that deep learning and deep neural networks have become so\npopular despite their simplicity? If there is no theoretical foundation\nto build on, why are they so \u003Cem\u003Eunreasonably\u003C\u002Fem\u003E efficient?\u003C\u002Fp\u003E\n\u003Cp\u003EIn this essay, I will argue the answer is: part coincidence, part skill.\nMotivating the skillfulness, I will first define deep learning (DL) and\nattempt to explain the phenomenon from a first-principle&#39;s approach. In\nthis light, I will describe the &quot;coincidental&quot; part by revisiting sets\nof problems DL is particularly good at, so we can finally arrive at an\nanswer to our question: why are deep networks so useful?\u003C\u002Fp\u003E\n\u003Ch1 id=\"deep-learning\"\u003EDeep Learning\u003C\u002Fh1\u003E\n\u003Cp\u003EEven though many DL proponents promote its similarities to the human\nbrain, the building block of deep networks have little to do with\nnervous systems. Deep networks can be roughly defined as large\ncomputational graphs with weighted nonlinearities.\u003C\u002Fp\u003E\n\u003Cp\u003EThis [\u003Ca href=\"202111_unreasonable_effectiveness\u002F#3\"\u003E4\u003C\u002Fa\u003E] is [\u003Ca href=\"202111_unreasonable_effectiveness\u002F#4\"\u003E5\u003C\u002Fa\u003E] not [\u003Ca href=\"202111_unreasonable_effectiveness\u002F#5\"\u003E6\u003C\u002Fa\u003E] new\n[\u003Ca href=\"202111_unreasonable_effectiveness\u002F#6\"\u003E7\u003C\u002Fa\u003E].\u003C\u002Fp\u003E\n\u003Cp\u003EIn fact, eerily similar approaches was attempted throughout 1960-1990,\nwith little success. What makes deep networks interesting, though, is\nthat they can approximate \u003Cem\u003Eany function\u003C\u002Fem\u003E to \u003Cem\u003Eany desired degree of\napproximation\u003C\u002Fem\u003E [\u003Ca href=\"202111_unreasonable_effectiveness\u002F#7\"\u003E8\u003C\u002Fa\u003E]. This striking fact was\ndiscovered in 1986 and tells us that deep networks can solve \u003Cem\u003Eany\u003C\u002Fem\u003E\nproblem, \u003Cem\u003Eif\u003C\u002Fem\u003E they are big enough. Granted, that is a big \u003Cem\u003Eif\u003C\u002Fem\u003E, and this\nhas largely been driving the explosive growth of the graphics processing\nunits (GPUs) market: the bigger network, the better approximations.\u003C\u002Fp\u003E\n\u003Cp\u003EIt still took several decades before the idea of a universal function\napproximator could be used to solve problems with any form of practical\nrelevance. Theoretically, this was already solved in 1986,\nwhere \u003Ca href=\"202111_unreasonable_effectiveness\u002F#6\"\u003E7\u003C\u002Fa\u003E\nshowed that large nets could be &quot;trained&quot; by \u003Cem\u003Esearching\u003C\u002Fem\u003E for optimal\nconfigurations in a sea of possibilities. However, it was not until the\nearly 2000&#39;s that sufficiently powerful hardware came along to build and\nsearch through massive deep networks now up to \u003Cem\u003Ehundreds of trillions\u003C\u002Fem\u003E\nof parameters [\u003Ca href=\"202111_unreasonable_effectiveness\u002F#2\"\u003E3\u003C\u002Fa\u003E].\u003C\u002Fp\u003E\n\u003Cfigure\u003E\n\u003Cimg src=\"cat.png\"\u002F\u003E\n\u003Cp class=\"caption\"\u003E\nFigure 1. Two possible classification task based on the same cat picture: \nobject identification (top) and masking out pixels belonging to the cat (bottom).\n\u003C\u002Fp\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Ch1 id=\"deep-learning-first-principles\"\u003EDeep Learning first principles\u003C\u002Fh1\u003E\n\u003Cp\u003EIt may come as a surprise how this type of searching yields meaningful\nnetworks. Fortunately, physicists and mathematicians helped resolve\nparts of the conundrum with two ancient ideas: symmetry\n[\u003Ca href=\"202111_unreasonable_effectiveness\u002F#8\"\u003E9\u003C\u002Fa\u003E] and equivariance [\u003Ca href=\"202111_unreasonable_effectiveness\u002F#9\"\u003E10\u003C\u002Fa\u003E].\u003C\u002Fp\u003E\n\u003Cp\u003EIt turns out that symmetry is a useful property during learning. If you\nare to, say, identify a cat in an image, it should not matter how big\nthe cat is. Or where in the picture the cat is. In other words, we\nwant to find a way to learn a way to look at cats that are \u003Cem\u003Esymmetric\u003C\u002Fem\u003E\nunder scale (size) or translation (location in picture). This symmetry\nis normally called \u003Cem\u003Einvariance\u003C\u002Fem\u003E, because the idea that a cat is in the\npicture \u003Cem\u003Edoes not vary\u003C\u002Fem\u003E if the cat is very large and dead-center, or \nreally small in the lower left corner. We identify cats independently\nfrom those \u003Cem\u003Edistortions\u003C\u002Fem\u003E, \u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmi\u003Eξ\u003C\u002Fmi\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003E\\xi\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.04601em;\"\u003Eξ\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E.\u003C\u002Fp\u003E\n\u003Cp\u003EWe can write this down more precisely by looking at some images\n\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmi\u003Ex\u003C\u002Fmi\u003E\u003Cmo\u003E∈\u003C\u002Fmo\u003E\u003Cmi mathvariant=\"double-struck\"\u003EI\u003C\u002Fmi\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Ex \\in \\mathbb{I}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.5782em;vertical-align:-0.0391em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\"\u003Ex\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mrel\"\u003E∈\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.6889em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathbb\"\u003EI\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\nthat we are looking to classify into a binary decision\n\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmi\u003Ey\u003C\u002Fmi\u003E\u003Cmo\u003E∈\u003C\u002Fmo\u003E\u003Cmi mathvariant=\"double-struck\"\u003EC\u003C\u002Fmi\u003E\u003Cmo\u003E:\u003C\u002Fmo\u003E\u003Cmo stretchy=\"false\"\u003E{\u003C\u002Fmo\u003E\u003Cmtext\u003ECAT\u003C\u002Fmtext\u003E\u003Cmo separator=\"true\"\u003E,\u003C\u002Fmo\u003E\u003Cmi mathvariant=\"normal\"\u003E¬\u003C\u002Fmi\u003E\u003Cmtext\u003ECAT\u003C\u002Fmtext\u003E\u003Cmo stretchy=\"false\"\u003E}\u003C\u002Fmo\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Ey \\in \\mathbb{C}: \\{\\text{CAT}, \\neg\\text{CAT}\\}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.7335em;vertical-align:-0.1944em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003Ey\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mrel\"\u003E∈\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.6889em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathbb\"\u003EC\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mrel\"\u003E:\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mopen\"\u003E{\u003C\u002Fspan\u003E\u003Cspan class=\"mord text\"\u003E\u003Cspan class=\"mord\"\u003ECAT\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mpunct\"\u003E,\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E¬\u003C\u002Fspan\u003E\u003Cspan class=\"mord text\"\u003E\u003Cspan class=\"mord\"\u003ECAT\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mclose\"\u003E}\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E using\na network \u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmi\u003Ef\u003C\u002Fmi\u003E\u003Cmo\u003E:\u003C\u002Fmo\u003E\u003Cmi mathvariant=\"double-struck\"\u003EI\u003C\u002Fmi\u003E\u003Cmo\u003E↦\u003C\u002Fmo\u003E\u003Cmi mathvariant=\"double-struck\"\u003EC\u003C\u002Fmi\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Ef: \\mathbb{I} \\mapsto \\mathbb{C}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.10764em;\"\u003Ef\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mrel\"\u003E:\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.6999em;vertical-align:-0.011em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathbb\"\u003EI\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mrel\"\u003E↦\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.6889em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathbb\"\u003EC\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, we can therefore say that\nthe guess \u003Cem\u003Ewithout\u003C\u002Fem\u003E a distortion \u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmi\u003Eξ\u003C\u002Fmi\u003E\u003Cmo\u003E:\u003C\u002Fmo\u003E\u003Cmi mathvariant=\"double-struck\"\u003EI\u003C\u002Fmi\u003E\u003Cmo\u003E↦\u003C\u002Fmo\u003E\u003Cmi mathvariant=\"double-struck\"\u003EI\u003C\u002Fmi\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003E\\xi: \\mathbb{I} \\mapsto \\mathbb{I}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.04601em;\"\u003Eξ\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mrel\"\u003E:\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.6999em;vertical-align:-0.011em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathbb\"\u003EI\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mrel\"\u003E↦\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.6889em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathbb\"\u003EI\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E,\nshould be the same \u003Cem\u003Ewith\u003C\u002Fem\u003E the distortion: \u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmi\u003Ef\u003C\u002Fmi\u003E\u003Cmo stretchy=\"false\"\u003E(\u003C\u002Fmo\u003E\u003Cmi\u003Ex\u003C\u002Fmi\u003E\u003Cmo stretchy=\"false\"\u003E)\u003C\u002Fmo\u003E\u003Cmo\u003E=\u003C\u002Fmo\u003E\u003Cmi\u003Ef\u003C\u002Fmi\u003E\u003Cmo stretchy=\"false\"\u003E(\u003C\u002Fmo\u003E\u003Cmi\u003Eξ\u003C\u002Fmi\u003E\u003Cmo stretchy=\"false\"\u003E(\u003C\u002Fmo\u003E\u003Cmi\u003Ex\u003C\u002Fmi\u003E\u003Cmo stretchy=\"false\"\u003E)\u003C\u002Fmo\u003E\u003Cmo stretchy=\"false\"\u003E)\u003C\u002Fmo\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Ef(x) = f(\\xi (x))\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.10764em;\"\u003Ef\u003C\u002Fspan\u003E\u003Cspan class=\"mopen\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\"\u003Ex\u003C\u002Fspan\u003E\u003Cspan class=\"mclose\"\u003E)\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mrel\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.10764em;\"\u003Ef\u003C\u002Fspan\u003E\u003Cspan class=\"mopen\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.04601em;\"\u003Eξ\u003C\u002Fspan\u003E\u003Cspan class=\"mopen\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\"\u003Ex\u003C\u002Fspan\u003E\u003Cspan class=\"mclose\"\u003E))\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EWe can take this one step further by generalizing the invariance, so\nthat the networks can operate on distorted images, \u003Cem\u003Ewhile retaining the\ndistortion\u003C\u002Fem\u003E [\u003Ca href=\"202111_unreasonable_effectiveness\u002F#9\"\u003E10\u003C\u002Fa\u003E]. Imagine for a while, that we are no\nlonger identifying cats, but colouring them, as shown in Figure 1. In\nthat case, invariance will not do because we need to \u003Cem\u003Einclude the\ndistortion in the output\u003C\u002Fem\u003E. Relating this to the example of the cat, we\ncannot always think that the cat is in the center of the image and\ncolour its outline there. Rather, we need to correctly scale and\ntranslate the outline in the output image, that is, we need to \u003Cem\u003Epreserve\nthe structure\u003C\u002Fem\u003E of the distortion. \nWhen we want to mask our cat (as in Figure 1), our \u003Cem\u003Estructure preserving\u003C\u002Fem\u003E\nnetwork should be written as \u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmi\u003Ef\u003C\u002Fmi\u003E\u003Cmo\u003E:\u003C\u002Fmo\u003E\u003Cmi mathvariant=\"double-struck\"\u003EI\u003C\u002Fmi\u003E\u003Cmo\u003E↦\u003C\u002Fmo\u003E\u003Cmi mathvariant=\"double-struck\"\u003EI\u003C\u002Fmi\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Ef: \\mathbb{I} \\mapsto \\mathbb{I}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.10764em;\"\u003Ef\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mrel\"\u003E:\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.6999em;vertical-align:-0.011em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathbb\"\u003EI\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mrel\"\u003E↦\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.6889em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathbb\"\u003EI\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \nfulfill\u003Ca href=\"#footnote-1\" class=\"footnote-a\"\u003E1\u003C\u002Fa\u003E: \u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmi\u003Eξ\u003C\u002Fmi\u003E\u003Cmo stretchy=\"false\"\u003E(\u003C\u002Fmo\u003E\u003Cmi\u003Ef\u003C\u002Fmi\u003E\u003Cmo stretchy=\"false\"\u003E(\u003C\u002Fmo\u003E\u003Cmi\u003Ex\u003C\u002Fmi\u003E\u003Cmo stretchy=\"false\"\u003E)\u003C\u002Fmo\u003E\u003Cmo stretchy=\"false\"\u003E)\u003C\u002Fmo\u003E\u003Cmo\u003E=\u003C\u002Fmo\u003E\u003Cmi\u003Ef\u003C\u002Fmi\u003E\u003Cmo stretchy=\"false\"\u003E(\u003C\u002Fmo\u003E\u003Cmi\u003Eξ\u003C\u002Fmi\u003E\u003Cmo stretchy=\"false\"\u003E(\u003C\u002Fmo\u003E\u003Cmi\u003Ex\u003C\u002Fmi\u003E\u003Cmo stretchy=\"false\"\u003E)\u003C\u002Fmo\u003E\u003Cmo stretchy=\"false\"\u003E)\u003C\u002Fmo\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003E\\xi(f(x)) = f(\\xi(x))\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.04601em;\"\u003Eξ\u003C\u002Fspan\u003E\u003Cspan class=\"mopen\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.10764em;\"\u003Ef\u003C\u002Fspan\u003E\u003Cspan class=\"mopen\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\"\u003Ex\u003C\u002Fspan\u003E\u003Cspan class=\"mclose\"\u003E))\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mrel\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.10764em;\"\u003Ef\u003C\u002Fspan\u003E\u003Cspan class=\"mopen\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.04601em;\"\u003Eξ\u003C\u002Fspan\u003E\u003Cspan class=\"mopen\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\"\u003Ex\u003C\u002Fspan\u003E\u003Cspan class=\"mclose\"\u003E))\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EOur network is now a \u003Cem\u003Estructure preserving map\u003C\u002Fem\u003E, also known as \u003Ci\u003E\u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FEquivariant_map\"\u003Eequivariant maps\u003C\u002Fa\u003E\u003C\u002Fi\u003E.\nThey are fundamental to, for instance, topology and graph theory.\u003C\u002Fp\u003E\n\u003Ch1 id=\"suitable-problems-for-deep-learning\"\u003ESuitable problems for Deep Learning\u003C\u002Fh1\u003E\n\u003Cp\u003EIt has been shown that deep networks are particularly good at preserving\nsymmetry and structures [\u003Ca href=\"202111_unreasonable_effectiveness\u002F#9\"\u003E10\u003C\u002Fa\u003E; \u003Ca href=\"202111_unreasonable_effectiveness\u002F#8\"\u003E9\u003C\u002Fa\u003E]. This\nkind of preservation allows for the detection of symmetries and\nstructures, but also the \u003Cem\u003Emapping\u003C\u002Fem\u003E of symmetries and structures between\ndomains such as image, 3d-space, language, audio and many others\n[\u003Ca href=\"202111_unreasonable_effectiveness\u002F#10\"\u003E11\u003C\u002Fa\u003E].\u003C\u002Fp\u003E\n\u003Cp\u003EThis is obviously relevant for classifying and masking cats, but what about\neverything else &quot;AI&quot;? What about other types of\nintelligence like bodily intelligences, causal reasoning (what is the\n\u003Cem\u003Esource\u003C\u002Fem\u003E of the structure), abstract thinking, and so on? Is that all a\nmatter of preserving mapping? \nMaybe. We do not know [\u003Ca href=\"202111_unreasonable_effectiveness\u002F#11\"\u003E12\u003C\u002Fa\u003E]. Please find out.\u003C\u002Fp\u003E\n\u003Cfigure\u003E\n\u003Cimg src=\"ai_pop.png\" style=\"width: 80%;\"\u002F\u003E\n\u003Cp class=\"caption\"\u003E\nPopularity of the search terms \"deep learning\" and \"artificial\nintelligence\" on Google Trends [\u003Ca href=\"202111_unreasonable_effectiveness\u002F#12\"\u003E13\u003C\u002Fa\u003E]. The Y-axis expresses\nrelative popularity where 100 indicates the peak number of\nsearchers.\n\u003C\u002Fp\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Ch1 id=\"why-did-deep-learning-become-so-popular\"\u003EWhy did Deep Learning become so popular?\u003C\u002Fh1\u003E\n\u003Cp\u003EReturning to the idea of a universal function approximator, these\nnetworks are precisely exercising the approximation of domain mappings.\nThey find appropriate responses such as &quot;CAT&quot;, a colouring mask, or even\na synthesized voice command. Pause here for a second and imagine how\nmany problems can be solved this way. Is language a mapping of concepts\nto voice or text [\u003Ca href=\"202111_unreasonable_effectiveness\u002F#2\"\u003E3\u003C\u002Fa\u003E]? Is programming a mapping of concepts to\ncode [\u003Ca href=\"202111_unreasonable_effectiveness\u002F#13\"\u003E14\u003C\u002Fa\u003E]?\u003C\u002Fp\u003E\n\u003Cp\u003EBy now, the reader is hopefully convinced of the usefulness of the\napproach. And plenty of evidence exists in applications for speech\nsynthesis, self-driving cars, facial recognition, object detection, \n\u003Ca href=\"https:\u002F\u002Fartsandculture.google.com\u002Fexperiment\u002Fblob-opera\u002FAAHWrq360NcGbw\"\u003Efake blob opera\u003C\u002Fa\u003E, etc.\nThis is the coincidence: pioneers of the abovementioned principles could\nnot possibly have known the extend to which their findings would\ngeneralize.\u003C\u002Fp\u003E\n\u003Cp\u003EFigure 2 plots the relative popularity of two search terms: &quot;artificial\nintelligence&quot; and &quot;deep learning&quot; [\u003Ca href=\"202111_unreasonable_effectiveness\u002F#12\"\u003E13\u003C\u002Fa\u003E]. It appears the general\npublic lost interest in &quot;classical&quot; AI in the beginning of the 00&#39;s only\nto slowly move towards deep learning-driven AI in the mid 10&#39;s.\nInterestingly enough, this coincides with the first papers on\ninvariances and structure preservations in 2014\u002F15.\u003C\u002Fp\u003E\n\u003Cp\u003EIt would be presumptuous to exclude other factors, and the advent of\ncommercialized hardware that unlocks the training of large deep nets, is\ncertainly one such influence. But the fundamental properties we have\ndiscussed here, are not only interesting because they explain what is\nfeasible, they also hint at what is \u003Cem\u003Enot\u003C\u002Fem\u003E. Perhaps that is why deep\nlearning is losing popularity towards the end of 2020?\u003C\u002Fp\u003E\n\u003Cdiv class=\"footnotes\"\u003E\n[ \u003Cb id=\"footnote-1\"\u003E1\u003C\u002Fb\u003E ]: This is also known as *equivariance* and the formula is slightly\n    more involved, since the distortion in the input domain may be\n    different from the distortion in the codomain. See\n    [\u003Ca href=\"202111_unreasonable_effectiveness\u002F#10\"\u003E11\u003C\u002Fa\u003E] for a more detailed\n    treatment.\n\n\u003C\u002Fdiv\u003E\n\u003Ch2\u003EReferences\u003C\u002Fh2\u003E\u003Col class='references'\u003E\u003Cli id=\"0\"\u003E\u003Ca href=\"http:\u002F\u002Faima.cs.berkeley.edu\u002F\"\u003ERussel, Stuart and Norvig, Peter: \u003Ci\u003EArtificial Intelligence: A Modern Approach, 4th US ed.\u003C\u002Fi\u003E, 2010, Pearson\u003C\u002Fa\u003E\u003C\u002Fli\u003E\u003Cli id=\"1\"\u003E\u003Ca href=\"https:\u002F\u002Fdoi.org\u002F10.1038\u002Fnature24270\"\u003ESilver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis: \u003Ci\u003EMastering the game of Go without human knowledge\u003C\u002Fi\u003E, 2017, Nature\u003C\u002Fa\u003E\u003C\u002Fli\u003E\u003Cli id=\"2\"\u003E\u003Ca href=\"http:\u002F\u002Farxiv.org\u002Fabs\u002F2005.14165\"\u003EBrown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario: \u003Ci\u003ELanguage Models are Few-Shot Learners\u003C\u002Fi\u003E, 2020, arXiv:2005.14165 [cs]\u003C\u002Fa\u003E\u003C\u002Fli\u003E\u003Cli id=\"3\"\u003E\u003Ca href=\"https:\u002F\u002Fdoi.org\u002F10.1002\u002Fj.1538-7305.1925.tb03972.x\"\u003ECarson, John R.: \u003Ci\u003EElectric circuit theory and the operational calculus\u003C\u002Fi\u003E, 1925, The Bell System Technical Journal\u003C\u002Fa\u003E\u003C\u002Fli\u003E\u003Cli id=\"4\"\u003E\u003Ca href=\"https:\u002F\u002Fieeexplore.ieee.org\u002Fdocument\u002F5311556\"\u003EClaude E. Shannon: \u003Ci\u003EClaude E. Shannon: Collected Papers\u003C\u002Fi\u003E, 1993, IEEE\u003C\u002Fa\u003E\u003C\u002Fli\u003E\u003Cli id=\"5\"\u003E\u003Ca href=\"https:\u002F\u002Fdoi.org\u002F10.1037\u002Fh0042519\"\u003ERosenblatt, F.: \u003Ci\u003EThe perceptron: A probabilistic model for information storage and organization in the brain\u003C\u002Fi\u003E, 1958, Psychological Review\u003C\u002Fa\u003E\u003C\u002Fli\u003E\u003Cli id=\"6\"\u003E\u003Ca href=\"https:\u002F\u002Fdoi.org\u002F10.1038\u002F323533a0\"\u003ERumelhart, D. and Hinton, Geoffrey E. and Williams, Ronald J.: \u003Ci\u003ELearning representations by back-propagating errors\u003C\u002Fi\u003E, 1986, Nature\u003C\u002Fa\u003E\u003C\u002Fli\u003E\u003Cli id=\"7\"\u003E\u003Ca href=\"https:\u002F\u002Fdoi.org\u002F10.1016\u002F0893-6080(89)90020-8\"\u003EHornik, Kurt and Stinchcombe, Maxwell and White, Halbert: \u003Ci\u003EMultilayer feedforward networks are universal approximators\u003C\u002Fi\u003E, 1989, Neural Networks\u003C\u002Fa\u003E\u003C\u002Fli\u003E\u003Cli id=\"8\"\u003E\u003Ca href=\"https:\u002F\u002Fpapers.nips.cc\u002Fpaper\u002F2014\u002Fhash\u002Ff9be311e65d81a9ad8150a60844bb94c-Abstract.html\"\u003EGens, Robert and Domingos, Pedro M: \u003Ci\u003EDeep Symmetry Networks\u003C\u002Fi\u003E, 2014, Advances in Neural Information Processing Systems\u003C\u002Fa\u003E\u003C\u002Fli\u003E\u003Cli id=\"9\"\u003E\u003Ca href=\"http:\u002F\u002Farxiv.org\u002Fabs\u002F1602.07576\"\u003ECohen, Taco S. and Welling, Max: \u003Ci\u003EGroup Equivariant Convolutional Networks\u003C\u002Fi\u003E, 2016, arXiv:1602.07576 [cs, stat]\u003C\u002Fa\u003E\u003C\u002Fli\u003E\u003Cli id=\"10\"\u003E\u003Ca href=\"http:\u002F\u002Farxiv.org\u002Fabs\u002F2104.13478\"\u003EBronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar: \u003Ci\u003EGeometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges\u003C\u002Fi\u003E, 2021, arXiv:2104.13478 [cs, stat]\u003C\u002Fa\u003E\u003C\u002Fli\u003E\u003Cli id=\"11\"\u003E\u003Ca href=\"https:\u002F\u002Fdoi.org\u002F10.1038\u002Fs41583-020-00395-8\"\u003ESaxe, Andrew and Nelli, Stephanie and Summerfield, Christopher: \u003Ci\u003EIf deep learning is the answer, what is the question?\u003C\u002Fi\u003E, 2021, Nature Reviews Neuroscience\u003C\u002Fa\u003E\u003C\u002Fli\u003E\u003Cli id=\"12\"\u003EGoogle: \u003Ci\u003EGoogle Trends\u003C\u002Fi\u003E, 2021, undefined\u003C\u002Fli\u003E\u003Cli id=\"13\"\u003E\u003Ca href=\"https:\u002F\u002Fdoi.org\u002F10.1109\u002FSoutheastCon42311.2019.9020360\"\u003ETiwang, Raymond and Oladunni, Timothy and Xu, Weifeng: \u003Ci\u003EA Deep Learning Model for Source Code Generation\u003C\u002Fi\u003E, 2019, 2019 SoutheastCon\u003C\u002Fa\u003E\u003C\u002Fli\u003E\u003C\u002Fol\u003E"}}]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');(function(){try{eval("async function x(){}");var main="/client/client.7949dabb.js"}catch(e){main="/client/legacy/client.080d2cb2.js"};var s=document.createElement("script");try{new Function("if(0)import('')")();s.src=main;s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@2.0.5.js";s.setAttribute("data-main",main);}document.head.appendChild(s);}());</script> 