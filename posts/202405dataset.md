---
title: Event-based vision data generator
pubdate: 2024/05/12
image: https://github.com/ncskth/event-generator/raw/main/affine.gif
---

We released [a library for generating event-based vision data](https://github.com/ncskth/event-generator) with a host of features.
Our work can be used to generate data for training and testing event-based vision algorithms, under controlled conditions, and with a high degree of flexibility.

Specifically, we render moving shapes (by default we provide squares, triangles, and circles) in a 2-dimensional grid.
The output is a sequence of sparse frames that shows the positive and/or negative events generated by the **difference** of the shapes in consecutive frames.
This doesn't exactly simulate an event-camera because it doesn't model the exact circuitry in the camera, but it's a good approximation---and it's completely controllable.
In turn, this allows us to carefully construct our simulation environment to test our algorithms under exact conditions.

Specifically, you configure:
* The transformations the shapes are subject to
  * Everything up to [affine transformations](https://en.wikipedia.org/wiki/Affine_transformation) is supported, but you can choose to only use translation, scaling, rotation, shearing, or an arbitrary combination of these
* The amount of sparsity via the velocity of the shapes
    * The faster the shapes move, the more dense the output gets
* The amount of noise both in the rendering of the shapes and in the background
* The shapes you want to render
* And much more...

Please refer to the GitHub repository for more information: https://github.com/ncskth/event-generator

For further examples, see our paper on spatio-temporal covariance properties at [https://github.com/jegp/nrp](https://github.com/jegp/nrf).