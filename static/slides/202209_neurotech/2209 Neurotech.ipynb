{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b19cca3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###### <center><img src=\"norse_logo.png\" class=\"r-stretch\"/></center>\n",
    "<h1 style=\"font-size: 260%; margin: 0;\">Real-time neuromorphics with Norse</h1>\n",
    "<center style=\"width: 44%;\">\n",
    "    <ul style=\"list-style-type:none; float: left;\">\n",
    "        <li><h2>Jens Egholm Pedersen</h2></li>\n",
    "        <li><i class=\"fa fa-envelope\"/> <tt>&lt;jeped@kth.se&gt;</tt></li>\n",
    "        <li><i class=\"fa fa-twitter\"/> @jensegholm</li>\n",
    "        <li><i class=\"fa fa-home\"/> https://neurocomputing.systems</li>\n",
    "                                    </ul>\n",
    "</center>\n",
    "<center style=\"width: 52%; float: left; margin: 0 1em;\">\n",
    "    \n",
    "<br/>\n",
    "    <img src=\"ncs.png\" style=\"height: 200px; float: left; margin: 0 .1em;\";/>\n",
    "    <img src=\"kth.png\" style=\"height: 200px;float: left; margin: 0 .8em;\";/> \n",
    "    <img src=\"hbp.png\" style=\"height: 200px;float: left; margin: 0 .1em;\"  ;/> \n",
    "</center>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a32d7e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"norse_logo.png\" class=\"r-stretch\"/></center>\n",
    "<h1>Goal: Bring the success of machine learning to neuromorphics</h1>\n",
    "<table style=\"font-size: 140%;\">\n",
    "    <tr class=\"fragment\" style=\" background-color: transparent;\"><td style=\"text-align:right;\">5'</td><td colspan=2>Numerical acceleration of biological simulations</td></tr>\n",
    "    <tr class=\"fragment\" style=\" background-color: transparent;\"><td style=\"text-align:right;\">10'</td><td colspan=2>Neuromorphic engineering with Norse</td></tr>\n",
    "    <tr class=\"fragment\" style=\" background-color: transparent;\"><td style=\"text-align:right;vertical-align: top;\">30'</td><td colspan=2>Demonstration of real-time applications\n",
    "    <br/>\n",
    "    <div style=\"margin-left: 20%;\";>\n",
    "        Event-based camera processing <br/>\n",
    "        Closed-loop control systems\n",
    "    </div>\n",
    "    </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd2e1d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Numerical acceleration of biological simulations\n",
    "\n",
    "![](https://github.com/ncskth/norse-rl/raw/master/book/images/EnvAgentBrain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7bcafd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neurons as dynamical systems\n",
    "\n",
    "<br/>\n",
    "<div style=\"float: left; width: 36%; margin-right: 1em;\" class=\"fragment\">\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/48/Saltatory_Conduction.gif/1024px-Saltatory_Conduction.gif\" style=\"height: 600px; vertical-align: center;\"/>\n",
    "<span class=\"small\">\n",
    "        \n",
    "Source: [Wikipedia](https://en.wikipedia.org/wiki/Nervous_tissue#/media/File:Saltatory_Conduction.gif)\n",
    "        \n",
    "</span>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"float:left; width: 60%; vertical-align: center;\" class=\"fragment\">\n",
    "    <h3>Approximate models</h3>\n",
    "    <br/>\n",
    "Math: $\n",
    "\\dot{v} = \\tau_{mem}(v_{leak} - v + i)\n",
    "$\n",
    "   \n",
    "<br/>\n",
    "    \n",
    "<div class=\"fragment\">\n",
    "    \n",
    "Python: `delta_v = tau * (v_leak - old_v + i)`\n",
    "\n",
    "</div>\n",
    "    \n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "<h1 class=\"fragment\" style=\"text-align: center;\">How do we simulate that in real-time?</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83548b11",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neuromorphic hardware accelerators\n",
    "\n",
    "\n",
    "<div style=\"width: 35%; float: left; padding-right: 2em;\" class=\"fragment\">\n",
    "    <br/><br/>\n",
    "<img src=\"chiptobrain.png\" />\n",
    "</div>\n",
    "\n",
    "<div style=\"width: 63%; float:left; font-size: 120%;\" class=\"fragment\">\n",
    "    \n",
    "<br/>\n",
    "Many exciting neuromorphic platforms in development. But...\n",
    "<ul>\n",
    "    <li class=\"fragment\">No common interface or programming language</li>\n",
    "    <li class=\"fragment\">Many platforms does not allow training on-chip</li>\n",
    "    <li class=\"fragment\">Interoperability is difficult</li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78415c08",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Contemporary accelerators for machine learning\n",
    "\n",
    "<center style=\"vertical-align: middle; width: 40%; float: left;\" class=\"fragment\">\n",
    "<img src=\"pytorch.png\" style=\"width:400px;margin: 2em;\">\n",
    "<img src=\"tensorflow.png\" style=\"width:300px; margin: 3em;\">\n",
    "<img src=\"jax.png\" style=\"width:200px;margin: 2em;\">\n",
    "</center>\n",
    "\n",
    "<center style=\"float: left; width: 50%;\" class=\"fragment\">\n",
    "\n",
    "<h4>Evolution of the graphical processing unit (GPU)</h4>\n",
    "<img src=\"gpu.gif\" class=\"r-stretch\" style=\"background: white; padding: 1em;\"/>\n",
    "    \n",
    "<p style=\"text-align: center;\">\n",
    "Source: <a href=\"https://ieeexplore.ieee.org/document/9623445\">Dally et al.: Evolution of the GPU, IEEE Micro 2021</a>\n",
    "    </p>\n",
    "\n",
    "</center>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7752f022",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Can we exploit modern machine learning accelerators?\n",
    "\n",
    "![](https://github.com/ncskth/norse-rl/raw/master/book/images/EnvAgentBrain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d5dad4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<h1>Yes <span class=\"fragment\">- and they are compatible with neuromorphic hardware</span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a652fea",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# An alliance between deep learning and neuromorphic hardware\n",
    "\n",
    "<br/>\n",
    "<div style=\"width: 35%; float: left;\">\n",
    "<img src=\"norsetochiptobrain.png\" style=\"height: 60%;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"width: 60%; float:left; font-size: 120%;\">\n",
    "    <br/>\n",
    "<ul>\n",
    "    <li class=\"fragment\">We virtualize biological computation</li>\n",
    "    <li class=\"fragment\">Train models on conventional hardware</li>\n",
    "    <li class=\"fragment\">Transfer models to neuromorphic hardware <br/><span class=\"fragment\">(<a href=\"https://en.wikipedia.org/wiki/Meta_learning\">Similar to meta-learning</a>)</span></li>\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac68c043",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"norse_logo.png\" class=\"r-stretch\"/></center>\n",
    "\n",
    "<h1>2. Neuromorphic engineering with Norse</h1>\n",
    "\n",
    "<h2>Goal: Bring the success of machine learning to neuromorphics</h2>\n",
    "\n",
    "<ol>\n",
    "    <li class=\"fragment\">Define biological neuron models</li>\n",
    "    <li class=\"fragment\">Simulate models in time</li>\n",
    "    <li class=\"fragment\">Accelerating our simulation</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13435cd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2.1 Defining biological neuron models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e073fd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Math: $\\dot{v} = \\tau_{mem}(v_{leak} - v + i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbcb263",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Python: `delta_v = tau * (v_leak - old_v + i)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d519e583",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import norse.torch as snn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb5563c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adc13ca6",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "`import norse.torch as snn\n",
    "snn.LICell()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "318061b4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LICell(p=LIParameters(tau_syn_inv=tensor(200.), tau_mem_inv=tensor(100.), v_leak=tensor(0.)), dt=0.001)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snn.LICell()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ade48",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "`snn.LIFCell()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3b8548",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Defining our data\n",
    "\n",
    "Norse is built on PyTorch who works with **tensors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec173de7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba29d560",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "`torch.tensor([0])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c10fb2f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Can be any dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2c9982",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "`data = torch.ones(10)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76b3b7b7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d7fd73",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Spikes are represented as `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41398d5d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65273f8e",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "`data = torch.ones(10)`\n",
    "\n",
    "Could be anything: vectors, matrices (cameras), high-dimensional data such as EEG recordings, etc.\n",
    "\n",
    "We can check the shape with `.shape`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6476708",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Applying our data to our model\n",
    "\n",
    "We can now take our **data** (tensor) and apply it to our **model** (neuron dynamics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb95de8e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.], grad_fn=<AddBackward0>),\n",
       " LIState(v=tensor([0.], grad_fn=<AddBackward0>), i=tensor([1.])))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.ones(1)\n",
    "model = snn.LICell()\n",
    "model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73819c8d",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "`snn.LIFCell()(torch.ones(1))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2af0ab",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Neurons are **stateful**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa4adfa0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LIState(v=tensor([0.], grad_fn=<AddBackward0>), i=tensor([1.]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, state =model(data)\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d566638",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "`output_values, output_state = snn.LICell()(data)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea72779a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data and neuron state\n",
    "\n",
    "<br/><br/>\n",
    "<img src=\"state.png\" style=\"width: 400px; float: left;\" />\n",
    "\n",
    "<img src=\"https://ncskth.github.io/norse-rl/_images/spikes.gif\" style=\"float: left; margin: auto 1em;\" class=\"fragment\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e8d606",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spiking models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ec9f0e4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.], grad_fn=<CppNode<SuperFunction>>),\n",
       " LIFFeedForwardState(v=tensor([0.], grad_fn=<AddBackward0>), i=tensor([1.])))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snn.LIFCell()(torch.ones(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dbfc68",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "`snn.LIFCell()(data)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcacd59",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2.2 Simulating our model - in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3406e97d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([0.], grad_fn=<CppNode<SuperFunction>>)\n",
      "tensor([1.], grad_fn=<CppNode<SuperFunction>>)\n"
     ]
    }
   ],
   "source": [
    "model = snn.LIFCell()\n",
    "data = torch.ones(1)\n",
    "state = None\n",
    "for t in range(100):\n",
    "    output, state = model(data, state)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c07f1e",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```model = snn.LIFCell()\n",
    "state = None\n",
    "data = torch.tensor(1)\n",
    "state = None\n",
    "for t in range(100):\n",
    "    model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0380fb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Using tensors with time\n",
    "\n",
    "Assuming time-series data, we can simulate it all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4352a2a0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]], grad_fn=<StackBackward0>),\n",
       " LIFFeedForwardState(v=tensor([0.], grad_fn=<AddBackward0>), i=tensor([5.0000])))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.ones(100, 1)\n",
    "snn.LIF()(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0519625",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```data = torch.ones(10, 1)\n",
    "model = snn.LIF()\n",
    "output, state = model(data)\n",
    "output```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677f8a8a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note the difference between `LIFCell` (without time) and `LIF`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc50747c",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "PyTorch convention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d4a9c9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2.3 Accelerating our model\n",
    "\n",
    "Our models have so far been running on the CPU - but it is simple to put them on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b66bf798",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = snn.SequentialState(\n",
    "  snn.Lift(torch.nn.Conv2d(1, 3, 5, padding=2)),\n",
    "  snn.LIF(),\n",
    "  torch.nn.Flatten(1),\n",
    "  torch.nn.Linear(30000, 10),\n",
    "  torch.nn.Sigmoid()\n",
    ")\n",
    "data = torch.ones(100, 1, 1, 100, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8347c59e",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```\n",
    "model = snn.SequentialState(\n",
    "    snn.Lift(torch.nn.Conv2d(1, 3, 5, padding=2)),\n",
    "    snn.LIF(),\n",
    "    torch.nn.Flatten(1),\n",
    "    torch.nn.Linear(30000, 10),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "data = torch.ones(100, 1, 1, 100, 100)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb97971a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Notice how this **merges machine learning and biological** models.\n",
    "  * This extends to complex biological phenomena such as plasticity and compartmental models\n",
    "  * More available on https://norse.github.io/norse/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0b62fd3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = snn.SequentialState(\n",
    "    snn.Lift(torch.nn.Conv2d(1, 3, 5, padding=2)),\n",
    "    snn.LIF(),\n",
    "    torch.nn.Flatten(1),\n",
    "    torch.nn.Linear(30000, 10),\n",
    "    torch.nn.Sigmoid()\n",
    ").cuda()\n",
    "data = torch.ones(100, 1, 1, 100, 100).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff04d91a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.4 ms ± 23.5 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a804472e",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "%timeit model(torch.ones(100, 1, 1, 100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7227509",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 ms ± 2.28 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "model = model.cpu()\n",
    "data = data.cpu()\n",
    "\n",
    "%timeit model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582d0250",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "model = model.cuda()\n",
    "data = data.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b216c5",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "%timeit model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0511415c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Norse performance\n",
    "<br/>\n",
    "<img src=\"benchmark_lif.png\" style=\"height: 70%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a15929",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Demonstration of real-time applications\n",
    "\n",
    "<img class=\"fragment\" src=\"https://github.com/ncskth/norse-rl/raw/master/book/images/EnvAgentBrain.png\" style=\"width: 80%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6788c6e6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Event-based camera processing\n",
    "2. Closed-loop control systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d34a36a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 3.1 Event-based camera processing\n",
    "\n",
    "Neuromorphic sensing is fundamentally different because they operate with **sparse** data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8e604d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Event-based cameras detect **luminosity <u>changes</u>** over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e71c83",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/26/Event_camera_comparison.jpg\" style=\"height: 400px;\"/>\n",
    "(Source: <a href=\"https://upload.wikimedia.org/wikipedia/commons/2/26/Event_camera_comparison.jpg\">Wikipedia</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fbdcdf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## **Task**: Identify horizontal and vertical edges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708d695c",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Sparse: event-based - only when an event occurs => fast and low processing\n",
    "* Frameless: high time resolution\n",
    "\n",
    "* Other benefits: high dynamic range, low energy consumption, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead0f773",
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 3.1 Event-based camera processing: edge detection\n",
    "\n",
    "<br/>\n",
    "<img src=\"2209_event_norse.png\" style=\"width: 60%;\"/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49764b75",
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Connect to event-based camera in Python\n",
    "2. Construct Norse model\n",
    "3. Feed event-camera into Norse model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1449aeb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3.1.1 Connecting to an event-based camera in Python\n",
    "\n",
    "We need an efficient tool: event cameras emit millions of events per second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8395e5e3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Aestream** - address-event streaming library: https://github.com/norse/aestream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eae285",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "import aestream\n",
    "with aestream.DVSInput((640, 480)) as dvs:\n",
    "    dvs.read() # Gives us a tensor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1094bde0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3.1.2 Construct Norse model\n",
    "\n",
    "<br/>\n",
    "\n",
    "Our model should"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26f6d2a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Detect either horizontal or vertical edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6d8d99",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. Filter out noisy pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abf5a05",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A simple convolutional edge detector\n",
    "\n",
    "By convolving our input signal with an edge detection kernel, we can single out oriented edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9c00724f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "kernel = (torch.linspace(-10, 10, 10).sigmoid().diff() - 0.14).repeat(9, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ed15db",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```field = torch.linspace(-10, 10, 10).sigmoid().diff() - 0.14\n",
    "kernel = field.repeat(9, 1)\n",
    "plt.imshow(kernel)```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a6e39",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Creating a convolutional PyTorch layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9342bf8d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "kernels = torch.stack([kernel, kernel.T])\n",
    "convolution = torch.nn.Conv2d(1, 2, 9)\n",
    "convolution.weight = torch.nn.Parameter(kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbea45b",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```kernels = torch.stack(kernel)\n",
    "convolution = torch.nn.Conv2d(1, 2, 9)\n",
    "convolution = torch.nn.Parameter(kernels)```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ba3117",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Filtering out noisy pixels\n",
    "\n",
    "How do we remove noise?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f24f506",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4a/Action_potential.svg/1280px-Action_potential.svg.png\" style=\"height: 50%;\"/>\n",
    "\n",
    "(Source: <a href=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4a/Action_potential.svg/1280px-Action_potential.svg.png\">Wikipedia</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b7b77",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Can we use the refractory period?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec74802",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modelling a simple noise filter with refractory neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb593d65",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "noise_filter = snn.LIFRefracCell()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe65f480",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "`noise_filter = snn.LIFRefracCell()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e3355d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Putting it together: an edge detector in Norse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5be085b0",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = snn.SequentialState(\n",
    "  noise_filter,\n",
    "  convolution\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd63d8c",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```model = snn.SequentialState(\n",
    "   noise_filter,\n",
    "   convolution\n",
    ")```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cdd39d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3.1.3 Feeding event-based camera into the model\n",
    "\n",
    "<br/>\n",
    "<img src=\"2209_event_norse.png\" style=\"width: 60%;\"/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ec1498",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "state = None\n",
    "with aestream.DVSInput((640, 480)) as dvs:\n",
    "    data = dvs.read()\n",
    "    output, state = model(data, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f97fca",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```python\n",
    "\n",
    "model = snn.SequentialState(\n",
    "   noise_filter,\n",
    "   convolution\n",
    ")\n",
    "\n",
    "with aestream.DVSInput((640, 480)) as dvs:\n",
    "    # Read the data\n",
    "    data = dvs.read()\n",
    "    # Run the model (with the previous state)\n",
    "    output, state = model(data, state)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8aee39",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 3.1 Event-based edge detector with Norse and aestream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "74e7e6a8",
   "metadata": {
    "code_folding": [],
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"middle\">\n",
       "<video width=\"80%\" controls autoplay loop>\n",
       "      <source src=\"2209-edge-detection.mp4\" type=\"video/mp4\">\n",
       "</video></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<div align=\"middle\">\n",
    "<video width=\"80%\" controls autoplay loop>\n",
    "      <source src=\"2209-edge-detection.mp4\" type=\"video/mp4\">\n",
    "</video></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c59717",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Full code is available online: [https://github.com/norse/aestream](https://github.com/norse/aestream/blob/main/example/usb_edgedetection.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3870c75",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 3.2 Real-time neural control system with Norse\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://github.com/ncskth/norse-rl/raw/master/book/images/EnvAgentBrain.png\" style=\"width: 52%; float: left; margin: 0 2%;\" class=\"fragment\"/>\n",
    "\n",
    "<div class=\"fragment\" style=\"float: left; width: 42%;margin: -14px 1rem;\">\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/38/Braitenberg_Vehicle_2ab.png\" style=\"height: 290px;\"/>\n",
    "\n",
    "Braitenberg vehicle\n",
    "    \n",
    "(Source: [Wikipedia](https://upload.wikimedia.org/wikipedia/commons/3/38/Braitenberg_Vehicle_2ab.png) )\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f835ae51",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Durin robot\n",
    "\n",
    "<div style=\"width: 40%; float: left;\">\n",
    "<br/>    \n",
    "    \n",
    "https://github.com/ncskth/durin\n",
    "    \n",
    "* Time-of-Flight (ToF) sensors\n",
    "* Four omnidirectional wheels\n",
    "    \n",
    "</div>\n",
    "\n",
    "<img src=\"https://github.com/ncskth/durin/blob/main/durin/durin_birdseye.jpg?raw=true\" style=\"width: 30%; float: left;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481b3cd8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Read ToF sensor data\n",
    "2. Construct a Norse model\n",
    "3. Send ToF sensor data to the Norse model and forward motor output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be02d150",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3.2.1 Read ToF sensor data\n",
    "\n",
    "<div style=\"width: 50%; float: left;\">\n",
    "\n",
    "Durin has 8 Time-of-Flight sensors, in an 8x8 array:\n",
    "    \n",
    "```python\n",
    "from durin import DurinUI\n",
    "\n",
    "with DurinUI(\"durin5.local\") as durin:\n",
    "    (obs, dvs, cmd) = durin.read()\n",
    "    obs.tof # Tof sensor data\n",
    "```\n",
    "\n",
    "<div class=\"fragment\">\n",
    "    \n",
    "    \n",
    "```python\n",
    "    left = obs.tof[1, 3, 3]\n",
    "```\n",
    "    \n",
    "</div>\n",
    "    \n",
    "\n",
    "<div class=\"fragment\">\n",
    "    \n",
    "    \n",
    "```python\n",
    "    right = obs.tof[6, 3, 3]\n",
    "```\n",
    "    \n",
    "</div>\n",
    "    \n",
    "    \n",
    "\n",
    "<div class=\"fragment\">\n",
    "    \n",
    "    \n",
    "```python\n",
    "    data = torch.tensor([left, right])\n",
    "```\n",
    "    \n",
    "</div>\n",
    "    \n",
    "</div>\n",
    "    \n",
    "    \n",
    "<img src=\"2209 durin.png\" style=\"width: 40%; float: left; margin-left: 6%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ce7707",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3.2.2 Construct a Norse model\n",
    "\n",
    "<div class=\"fragment\" style=\"float: left; width: 42%;margin: -14px 1rem;\">\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/38/Braitenberg_Vehicle_2ab.png\" style=\"height: 400px;\"/>\n",
    "    \n",
    "(Source: [Wikipedia](https://upload.wikimedia.org/wikipedia/commons/3/38/Braitenberg_Vehicle_2ab.png) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2b1a29b8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "p = snn.LIParameters(tau_mem_inv=100)\n",
    "network = snn.LICell(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab3a464",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```python\n",
    "p = snn.LIParameters(tau_mem_inv=100)\n",
    "network = snn.LICell(p)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcadcf37",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3.2.3 Send ToF sensor data to the Norse model and forward motor output\n",
    "\n",
    "<br/>\n",
    "\n",
    "<img src=\"2209 durin.png\" style=\"width: 300px; float: left; margin-right: 6%;\"/>\n",
    "\n",
    "<img src=\"state.png\" style=\"width: 300px; float: left;\" class=\"fragment\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66675e9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "state = None\n",
    "output, state = network(data, state)\n",
    "left, right = output\n",
    "command = MoveWheels(left, right, right, left)\n",
    "durin(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5e789f",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```python\n",
    "# Update the network with the input and read the output\n",
    "output, network_state = network(input_tensor, network_state)\n",
    "left, right = output # Unpack\n",
    "#                    NE     NW     SW     SE\n",
    "command = MoveWheels(left, right, right, left)\n",
    "durin(command)  # Send the command to Durin\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba06f01",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3.2.3 Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a59c8f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "from durin import DurinUI\n",
    "\n",
    "with DurinUI(\"durin5.local\") as durin:\n",
    "    # Extract tof sensor data\n",
    "    (obs, dvs, cmd) = durin.read()\n",
    "    left = obs.tof[1, 3, 3]\n",
    "    right = obs.tof[6, 3, 3]\n",
    "    data = torch.tensor([left, right])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2c3b3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "    # Update the network with the input and read the output\n",
    "    output, network_state = network(data, network_state)\n",
    "    left, right = output  # Separate left and right outputs\n",
    "    command = MoveWheels(left, right, right, left)\n",
    "    durin(command)  # Send the command to Durin\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47b5e01",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 3.2 Real-time neural control system with Norse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4018e330",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"middle\">\n",
       "<video width=\"80%\" controls autoplay loop>\n",
       "      <source src=\"2209-durin-braitenberg.mp4\" type=\"video/mp4\">\n",
       "</video></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<div align=\"middle\">\n",
    "<video width=\"80%\" controls autoplay loop>\n",
    "      <source src=\"2209-durin-braitenberg.mp4\" type=\"video/mp4\">\n",
    "</video></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17387c08",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Full example available here: [https://github.com/ncskth/durin/](https://github.com/ncskth/durin/blob/main/examples/braitenberg.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7974f8d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. Summary\n",
    "<br/>\n",
    "<img src=\"https://github.com/ncskth/norse-rl/raw/master/book/images/EnvAgentBrain.png\" style=\"width: 50%; float: left; margin: 0 2%;\" class=\"fragment\"/>\n",
    "<img src=\"norsetochiptobrain.png\" style=\"width: 30%; float: center;\" class=\"fragment\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb07718",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 4. Summary\n",
    "\n",
    "1. Learned to simulate and accelerate models with Norse and PyTorch\n",
    "2. Demonstrated event-based processing with Norse\n",
    "3. Demonstrated real-time neural control systems with Norse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e781e693",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Integration Norse with neuromorphic platforms\n",
    "\n",
    "\n",
    "<div style=\"width: 60%; float: left;\">\n",
    "    \n",
    "Work is ongoing to run Norse models on\n",
    "\n",
    "* SpiNNaker\n",
    "* BrainScales\n",
    "    \n",
    "<p class=\"fragment\">\n",
    "    <img src=\"ncs.png\" style=\"height: 300px; float: left; margin: 0 1em;\";/>\n",
    "</p>\n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "<img src=\"norsetochiptobrain.png\" style=\"width: 30%; float: center;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5972bf38",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 5. Future work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2aa3f7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Low-power, high time resolution neuromorphic control systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1bccce",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Gradient-based optimization of biological networks\n",
    "  * That are portable to neuromorphic hardware\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323e4172",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Meta-learning with outer (evolution) and inner (plasticity) training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434e94b4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* NeoHebbian, local plasticity rules directly in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661f341c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"norse_logo.png\" class=\"r-stretch\" style=\"height:260px;\"/>\n",
    "</center>\n",
    "<h1 style=\"font-size: 220%; margin: 0;\">Real-time neuromorphics with Norse</h1>\n",
    "\n",
    "<center style=\"width: 44%;\">\n",
    "    <ul style=\"list-style-type:none; float: left;\">\n",
    "        <li><h2>Jens Egholm Pedersen</h2></li>\n",
    "        <li><i class=\"fa fa-envelope\"/> <tt>&lt;jeped@kth.se&gt;</tt></li>\n",
    "        <li><i class=\"fa fa-twitter\"/> @jensegholm</li>\n",
    "        <li><i class=\"fa fa-home\"/> https://neurocomputing.systems</li>\n",
    "                                    </ul>\n",
    "</center>\n",
    "<center style=\"width: 52%; float: left; margin: 0 1em;\">\n",
    "    \n",
    "<br/>\n",
    "    <img src=\"ncs.png\" style=\"height: 200px; float: left; margin: 0 .1em;\";/>\n",
    "    <img src=\"kth.png\" style=\"height: 200px;float: left; margin: 0 .8em;\";/> \n",
    "    <img src=\"hbp.png\" style=\"height: 200px;float: left; margin: 0 .1em;\";/> \n",
    "</center>\n",
    "\n",
    "<br style=\"clear: both;\"/>\n",
    "    \n",
    "**Thank you!**\n",
    "    \n",
    "<ul style=\"margin-top: -0.5em;\">\n",
    "    <li>Christian Pehle and the BrainScales team at Heidelberg University.</li>\n",
    "    <li>Jörg Conradt, Juan P. Romero B., and our students at Neurocomputing Systems lab, KTH Royal Institute of Technology.</li>\n",
    "    <li>Luis Plana, Andrew Rowley, and the SpiNNaker team at Manchester University.</li>\n",
    "    <li>Human Brain Project for funding this work.</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
