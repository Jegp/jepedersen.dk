<!doctype html>
<html lang="en">

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Translation and scale invariance in event-based object tracking</title>

	<link rel="stylesheet" href="dist/reset.css">
	<link rel="stylesheet" href="dist/reveal.css">
	<link rel="stylesheet" href="dist/theme/serif.css">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="plugin/highlight/monokai.css">

	<link rel="stylesheet" href="rise.css">
</head>

<!-- Take away
- Spatio-temporal scale spaces are the future of event-based vision
-->

<body>
	<div class="reveal">
		<div class="slides">
			<section data-background-color="#fdfdfd">
				<video src="2209_event_video.mp4" loop autoplay muted controls />
				<aside class="notes">
					A bit more than a year ago, we wanted to train a spiking neural network to do simple object detection.
					Just tracking the shapes in the video you see here.

					We took a simple convolutional neural net, replaced the activation units with leaky integrate-and-fire units,
					and trained with backpropagation through time with surrogate gradients.

					So, a simple end-to-end neural network optimization problem.

					Except, it didn't work. So, we studied the problem and tried to find out what happened. Why wouldn't it learn?
				</aside>
			</section>
			<section>
				<h2>Event-based object tracking</h2>
				<h3>Translation and scale invariance in real-time with receptive fields</h3>
				<h4>Jens Pedersen &amp; Raghav Singhal &amp; Jörg Conradt</h4>
				<aside class="notes">
					The result of our analysis is the topic of this talk.

					I'll carry you through the insights that caused our model to fail, and discuss what we believe is the right
					approach to real-time high-performance event-based vision models.

					My name is Jens and together with Raghav and Jörg we have been working on formalizing invariance properties in
					event-based vision.
				</aside>
			</section>
			<section data-background-color="#1b2433">
				<section>
					<h2>What are events?</h2>
				</section>
				<section>
					<div class="r-stack">
						<div class="fragment fade-out r-stretch">
							<video src="pendulum.mp4" loop autoplay muted controls />
						</div>
						<img src="event_still.png" />
					</div>
					<aside class="notes">
						What you see here is a pendulum recorded with an event-based camera.
						Event cameras essentially record the <i>change</i> in luminosity above or below a certain threshold.

						As I'm sure you're aware, this is a fascinating technology, but models and algorithms for event-based vision
						are still struggling to keep up with conventional frame-based computer vision models.
						Unfortunately, we cannot directly transfer frame-based algorithms; if we take a single frame, we don't get
						much information.
					</aside>
				</section>
				<section>
					# Zoom in on a single pixel (ignore polarity)
					# A signal always have a temporal extent
				</section>
				<section>
					# Zoom in on a LI integration
				</section>
				<section>
					<h3>How do we build real-time event-based vision models?</h3>
					<ol>
						<li>Temporal integration of signals</li>
						<li>Stepwise real-time predictions</li>
					</ol>

					<aside class="notes">
						If we are to succeed in real-time event-based vision, I believe we need to tackle two problems
						<br>- We need some form of spatio-temporal integration to form an understanding about what an "event" is
						attached to
						<br>- We need predictions with millisecond precision to exploit the time granularity of event cameras
						We need some form of temporal integration or, recurrence, in the words of neural networks.

						In this talk, how to progress towards real-time event-based vision models, rivalling those
						from conventional computer vision.
					</aside>
				</section>
			</section>

			<section>
				<section>
					# Scale-space theory
				</section>

				<section>
					# Convolution gaussian 1 (translation invariance)
					# Convolution gaussian 2 (scale invariance)
				</section>

				<section>
					# Invariance properties
				</section>

				<section>
					# Scale spaces
				</section>

				<section>
					# Problem: Old networks couldn't converge on these invariance properties
				</section>
			</section>

			<section>
				<section>
					Real-time coordinate prediction
				</section>

				<section>
					<!-- TODO FIX MATH -->
					$$\int$$
					<img src="cr_heatmap.png" class="r-stretch" />
				</section>
			</section>

			<section>
				<section>
					<h2>Experimental setup &amp; results</h2>
				</section>
				<section data-background-color="#fdfdfd">
					<h3>1ms frames with coordinate labels</h3>
					<video src="2209_event_video.mp4" loop autoplay muted controls />
				</section>

				<section>
					<h3>Model with 4 scale spaces</h3>
					<img src="2212_NICE_model.svg" class="r-stretch" />
				</section>

				<section data-background-color="white">
					<img src="2212_cr_heatmap.png" class="r-stretch" />
					<aside class="notes">
						Three take-aways:
						<br>- ANN and SNN performs on-par
						<br>- ANN does not benefit from the receptive field. Fits with theory
						<br>- SNN does benefit, and halves the training time
					</aside>
				</section>

				<section>
					<h2>Runs at 1000Hz on GPUs</h2>
					<img src="pytorch.png" />
					<img src="norse_logo.png" />
				</section>
			</section>

			<section>
				<section data-transition="fade">
					<h2>Event-based object tracking</h2>
					<h3>Summary</h3>
					<ul>
						<li>SNN rivals ANN <b class="fragment">despite high density</b></li>
						<li class="fragment">Differentiable coordinate transformation</li>
						<li class="fragment"><b>Real-time vision processing with events</b></li>
					</ul>
				</section>
				<section data-transition="fade">
					<h2>Event-based object tracking</h2>
					<h3>Limitations</h3>
					<ul>
						<li>Only simulated data</li>
						<li>Only on GPUs</li>
						<li>Only translation and scale</li>
					</ul>
					<aside class="notes">
						Missing rotation, shear
					</aside>
				</section>

				<section data-transition="fade">
					<h2>Event-based object tracking</h2>
					<h3>Translation and scale invariance in real-time with receptive fields</h3>
					<h4>Jens Pedersen &amp; Raghav Singhal &amp; Jörg Conradt</h4>
				</section>
			</section>
		</div>
	</div>

	<script src="dist/reveal.js"></script>
	<script src="plugin/math/katex.js"></script>
	<script src="plugin/notes/notes.js"></script>
	<script src="plugin/markdown/markdown.js"></script>
	<script src="plugin/highlight/highlight.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			hash: true,
			slideNumber: "c/t",

			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [RevealMarkdown, RevealHighlight, RevealNotes]
		});
	</script>
</body>

</html>